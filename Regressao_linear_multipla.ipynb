{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af7081f",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60571d05",
   "metadata": {},
   "source": [
    "## Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca7c96",
   "metadata": {},
   "source": [
    "A ideia da regressão linear é tentar buscar uma reta que explica os targets a fim de prever uma saída dada uma determinada entrada, sendo ela uma reta (para 2d) ou um hiperplano, dado que:\n",
    "\n",
    "$$\n",
    "y(x) = \\beta0 + \\beta1x_1 $$\n",
    "\n",
    "\n",
    "### Minimos Quadrados\n",
    "Para determinar a melhor reta em 2d, é necessário achar os parâmetros que minimizam a soma dos erros quadraticos (RSS) $e = (yi - y)^2$, ou seja:\n",
    "\n",
    "$$\n",
    "\\beta1 = a = media_y - bx\n",
    "$$\n",
    "\n",
    "$$\n",
    "intercept = \\beta0 = b = \\frac{\\sum\\limits_{i = 1}^{n}(x_i - media_x)*(y_i - media_y)}{\\sum\\limits_{i = 1}^{n}(x_i - media_x)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<img src = 'imagens_regressao/regressao.png'>\n",
    "\n",
    "\n",
    "### Máxima verossimilhança\n",
    "\n",
    "A ideia de estimar os parâmetros usando a verossimilhança é fazer uma suposição que o **erro segue uma distribuição normal**, ou seja\n",
    "\n",
    "\n",
    "$$\n",
    "y(x) = ax + b + e\n",
    "$$\n",
    "$$\n",
    "e = N(0, variância)\n",
    "$$\n",
    "\n",
    "portanto\n",
    "\n",
    "$$\n",
    "y = N(ax + b, variância)\n",
    "$$\n",
    "\n",
    "<img src = 'imagens_regressao/vero.png'>\n",
    "\n",
    "ou seja, para cada $y_i, x_i$ existe um erro que segue uma normal e o objetivo é maxímizar a verossimilhança, ou seja, passar no meio da normal.\n",
    "\n",
    "<img src = 'imagens_regressao/vero1.png'>\n",
    "\n",
    "**Caso o target não apresente uma distribuição normal, como no exemplo abaixo, pode-se utilizar transformações logaritimas ou a transformação box cox $ x = \\frac{x^\\lambda - 1}{\\lambda}$**\n",
    "\n",
    "<img src = 'imagens_regressao/vero2.png'>\n",
    "\n",
    "\n",
    "### Gradiente Descendente\n",
    "\n",
    "A ideia do gradiente descendente é ajustar os parâmetros do modelo interativamente para minimizar a função de custo (erro). Ou seja, caminhar no sentido contrário do gradiente (derivada) da função de custo (MSE, MAE ...) para encontrar o conjunto de parâmetros tal que o gradiente seja nulo (ponto mínimo). \n",
    "\n",
    "<img src = 'imagens_regressao/gradiente.png'>\n",
    "\n",
    "A função de custo (MAE por exemplo) é dada por:\n",
    "\n",
    "$$\n",
    "e = (y - y_i)*x_i\n",
    "$$\n",
    "\n",
    "e por sua vez, a para a função 'aprender' é passada a taxa de aprendizagem, o qual irá ajustar os 'pesos' da função linear, ou seja:\n",
    "\n",
    "$$\n",
    "\\beta1{novo} = \\beta1{anterior} + \\:taxa\\:de\\:aprendizagem(erro_{atual}) \n",
    "$$\n",
    "\n",
    "<img src = 'imagens_regressao/taxa_de_aprendizagem.png'>\n",
    "\n",
    "\n",
    "Pode ocorrer uma variação da taxa de aprendizagem caso adicionemos a função de momentum na equação, o qual 'varia a taxa de aprendizagem' com a diferença na saída\n",
    "\n",
    "<img src = 'imagens_regressao/momentum.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8175512",
   "metadata": {},
   "source": [
    "## Regressão linear Multipla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e584a",
   "metadata": {},
   "source": [
    "$$\n",
    "y(x) = \\beta0 + \\beta1x_1 + \\beta2_x2 \\:...\\: + \\beta_nx_n\n",
    "$$\n",
    "\n",
    "Aplicando a derivada na função em cada um dos coeficientes $\\beta0, \\beta1 , \\beta2, \\:...\\: , \\beta_nx_n$ a gente chega na seguinte matriz, onde $n$ representa o numero de amostras para 2 dimensões\n",
    "\n",
    "\n",
    "<img src = 'imagens_regressao/multipla.png'>\n",
    "\n",
    "Um exemplo é: \n",
    "\n",
    "<img src = 'imagens_regressao/multipla_ex.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ba5f8",
   "metadata": {},
   "source": [
    "## Regressão Polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f21f73",
   "metadata": {},
   "source": [
    "Basicamente a ideia é a mesma da regressão multipla, derivando os coefientes 1 a 1, no qual resulta na matriz abaixo ($x^2$).\n",
    "\n",
    "<img src = 'imagens_regressao/regressao_poli.png'>\n",
    "\n",
    "<img src = 'imagens_regressao/regressao_poli1.png'>\n",
    "\n",
    "<img src = 'imagens_regressao/regressao_poli2.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
